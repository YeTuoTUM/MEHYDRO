{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40acb71b",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "[Decision Trees (DTs)](https://scikit-learn.org/stable/modules/tree.html#tree) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8165c",
   "metadata": {},
   "source": [
    "## A simple 1D regression with decision tree.\n",
    "The decision trees is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.\n",
    "\n",
    "We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3a6e3",
   "metadata": {},
   "source": [
    "### Create a random dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d67880",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dac35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28e3f8",
   "metadata": {},
   "source": [
    "## Same dataset with Random Forests\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to **improve the predictive accuracy and control over-fitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1163fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model\n",
    "regr_1 =  RandomForestRegressor(max_depth=2)\n",
    "regr_2 =  RandomForestRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Random Forest Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b80929",
   "metadata": {},
   "source": [
    "## Decision tree and random forests\n",
    "In this lab, we will apply decision tree and random forests to predict the latent heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79591d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('FLX_US-Ne1_FLUXNET2015_SUBSET_DD_2001-2013_1-4.csv' , delimiter=\",\", skipinitialspace=True,  parse_dates=True)\n",
    "\n",
    "meteo = pd.DataFrame(\n",
    "            {\"sw\": data.SW_IN_F, \"lw\": data.LW_IN_F, \"tmp\": data.TA_F,\n",
    "             \"pre\": data.PA_F, \"u10\": data.WS_F,  \"vpd\": data.VPD_F , \"lh\": data.LE_CORR})\n",
    "\n",
    "data_all = np.array(meteo)\n",
    "X = data_all[ : , 0:6]\n",
    "y = data_all[ : , 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b462da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(X):    \n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    tree1 = DecisionTreeRegressor(max_depth=5)\n",
    "    tree2 = RandomForestRegressor(max_depth=5)\n",
    "    tree1.fit(X_train , y_train)\n",
    "    tree2.fit(X_train , y_train)\n",
    "    y_pred1 = tree1.predict(X_test)\n",
    "    y_pred2 = tree2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "# Plot the data points\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "line = ax.plot( y_pred1, c='b',label='Decision Tree')\n",
    "plt.plot( y_pred2, marker='o', c='black',label='Random Forests')\n",
    "plt.plot( y_test, marker='x', c='r',label='True Value')\n",
    "# Set the y-axis label\n",
    "plt.ylabel('evaporation rate (mm/d)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('time (day)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 1, figsize=(6, 6), sharey=True)\n",
    "ax.scatter( y_test , y_pred1, c='b')\n",
    "z = np.polyfit(y_test , y_pred1, 1)\n",
    "y_hat = np.poly1d(z)(y_pred1)\n",
    "plt.plot(y_pred1, y_hat, \"r--\", lw=2)\n",
    "text = f\"$y={z[0]:0.3f}\\;x{z[1]:+0.3f}$\\n$R^2 = {r2_score(y_test, y_hat):0.3f}$\\n\" \\\n",
    "                   f\"$RMSE = {mean_squared_error(y_test, y_hat, squared=False):0.3f} $ \"\n",
    "plt.gca().text(0.05, 0.95, text, transform=plt.gca().transAxes,\n",
    "                           fontsize=14, verticalalignment='top')\n",
    "plt.title('Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd87a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 1, figsize=(6, 6), sharey=True)\n",
    "ax.scatter( y_test , y_pred2, c='b')\n",
    "z = np.polyfit(y_test , y_pred2, 1)\n",
    "y_hat = np.poly1d(z)(y_pred2)\n",
    "plt.plot(y_pred2, y_hat, \"r--\", lw=2)\n",
    "text = f\"$y={z[0]:0.3f}\\;x{z[1]:+0.3f}$\\n$R^2 = {r2_score(y_test, y_hat):0.3f}$\\n\" \\\n",
    "                   f\"$RMSE = {mean_squared_error(y_test, y_hat, squared=False):0.3f} $ \"\n",
    "plt.gca().text(0.05, 0.95, text, transform=plt.gca().transAxes,\n",
    "                           fontsize=14, verticalalignment='top')\n",
    "plt.ylabel('Predict Value')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('True Value')\n",
    "plt.title('Random Forests')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ebf05",
   "metadata": {},
   "source": [
    "## Please try leaning learning curve here according to jupyter notebook: regression_linear_polynomial.ipyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d52dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
