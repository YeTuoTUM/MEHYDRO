{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python with numpy\n",
    "### Problem statement: A brief introduction to Python and numpy\n",
    "In this part of this exercise, you will build up a sigmoid function, sigmoid gradient, normalizing rows and reshaping arrays by using numpy.\n",
    "\n",
    "**Purpose of this exercise:**\n",
    "- Be able to use numpy functions and numpy matrix/vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and print the version number\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numpy basic operations\n",
    "- Elementwise operations\n",
    "- Basic reductions\n",
    "- Broadcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise operations\n",
    "## With scalars  \n",
    "a = np.array([1, 2, 3, 4])\n",
    "print(a + 1)\n",
    "print(2**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All arithmetic operates elementwise \n",
    "b = np.ones(4) + 1\n",
    "print(a - b)\n",
    "print(a ** b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparisons\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([4, 2, 2, 4])\n",
    "print(a == b)\n",
    "print(a > b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logical operations\n",
    "a = np.array([1, 1, 0, 0], dtype=bool)\n",
    "b = np.array([1, 0, 1, 0], dtype=bool)\n",
    "print(np.logical_or(a, b))\n",
    "print(np.logical_and(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transcendental functions\n",
    "a = np.arange(1 , 5)\n",
    "print(np.sin(a))\n",
    "print(np.log(a))\n",
    "print(np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transposition\n",
    "a = np.triu(np.ones((3, 3)), 1)   # see help(np.triu)\n",
    "print(a)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic reductions\n",
    "## Computing sums\n",
    "x = np.array([1, 2, 3, 4])\n",
    "print(np.sum(x))\n",
    "print(x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum by rows and by columns\n",
    "x = np.array([[1, 1], [2, 2]])\n",
    "print(x)\n",
    "print(x.sum(axis=0))   # columns (first dimension)\n",
    "print(x[:, 0].sum(), x[:, 1].sum())\n",
    "print(x.sum(axis=1))   # rows (second dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extrema\n",
    "x = np.array([1, 3, 2])\n",
    "print(x.min())\n",
    "print(x.max())\n",
    "print(x.argmin())  # index of minimum\n",
    "print(x.argmax())  # index of maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistics\n",
    "x = np.array([1, 2, 3, 1])\n",
    "y = np.array([[1, 2, 3], [5, 6, 1]])\n",
    "print(x.mean())\n",
    "print(np.median(x))\n",
    "print(np.median(y, axis=-1)) # last axis\n",
    "print(x.std())          # full population standard dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "a = np.tile(np.arange(0, 40, 10), (3, 1)).T # Please check the np.tile\n",
    "print(a)\n",
    "b = np.array([0, 1, 2])\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((4, 5))\n",
    "a[0] = 2  # we assign an array of dimension 0 to an array of dimension 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0, 40, 10)\n",
    "print(a.shape)\n",
    "a = a[:, np.newaxis]  # adds a new axis -> 2D array\n",
    "print(a.shape)\n",
    "print(a)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sigmoid function \n",
    "\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the sigmoid function using numpy. \n",
    "\n",
    "# FUNCTION: sigmoid\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    y = np.exp(-x)\n",
    "    s = 1/(1+y)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.zeros((3 , 1))\n",
    "print(t_x)\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.random.random_sample((3 , 2))\n",
    "print(t_x)\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2) # Try different seeds\n",
    "t_x = np.random.random_sample((3 , 2))\n",
    "print(t_x)\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.arange((10))\n",
    "print(t_x)\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid Gradient\n",
    "Compute gradients to optimize loss functions using backpropagation.\n",
    "\n",
    "Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: \n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "You often code this function in two steps:\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # s = \n",
    "    # ds = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s= sigmoid(x)\n",
    "    ds = s*(1-s)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if \n",
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ \n",
    "then \n",
    "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "and\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: normalize_rows\n",
    "\n",
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # x_norm =\n",
    "    # Divide x by its norm.\n",
    "    # x =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n",
    "    x = x/x_norm\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Reshaping arrays\n",
    "\n",
    "Two common numpy functions used in deep learning are np.shape and np.reshape().\n",
    "\n",
    "    X.shape is used to get the shape (dimension) of a matrix/vector X.\n",
    "    X.reshape(...) is used to reshape X into some other dimension.\n",
    "\n",
    "Implement image2vector() that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "\n",
    "v = v.reshape((v.shape[0] * v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION:image2vector\n",
    "\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # v =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2],1))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "t_image = np.array([[[ 0.67,  0.29],\n",
    "                     [ 0.90,  0.52],\n",
    "                     [ 0.42 ,  0.45]],\n",
    "\n",
    "                   [[ 0.92,  0.96],\n",
    "                    [ 0.85,  0.52],\n",
    "                    [ 0.19,  0.27]],\n",
    "\n",
    "                   [[ 0.60,  0.01],\n",
    "                    [ 0.10,  0.49],\n",
    "                    [ 0.34,  0.94]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(t_image)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
